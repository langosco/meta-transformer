{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippets that load the meta-model.\n",
    "(And then you can test etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/meta-models/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.6\"  # preallocate a bit less memory so we can use pytorch next to jax\n",
    "\n",
    "from time import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from typing import List, Dict\n",
    "from jax.typing import ArrayLike\n",
    "from meta_transformer import utils, preprocessing, torch_utils, module_path, on_cluster, output_dir\n",
    "from meta_transformer.meta_model import MetaModel\n",
    "import wandb\n",
    "import argparse\n",
    "from dataclasses import asdict\n",
    "from meta_transformer.train import Updater, Logger\n",
    "from meta_transformer.data import data_iterator, split_data\n",
    "from etils import epath\n",
    "import haiku as hk\n",
    "from meta_transformer.transformer import Transformer\n",
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(rng, (1, 5, 10))\n",
    "\n",
    "try:\n",
    "    model = MetaModel(\n",
    "        d_model=400,\n",
    "        num_heads=2,\n",
    "        num_layers=1,\n",
    "        dropout_rate=0.1,\n",
    "        widening_factor=4,\n",
    "        use_embedding=True,\n",
    "    )\n",
    "\n",
    "except ValueError as err: # haiku\n",
    "    @hk.transform\n",
    "    def model(x, is_training):\n",
    "        model_fn = MetaModel(\n",
    "            model_size=400,\n",
    "            use_embedding=True,\n",
    "            transformer=Transformer(\n",
    "                num_layers=1,\n",
    "                num_heads=2,\n",
    "                dropout_rate=0.1,\n",
    "                key_size=400//2,\n",
    "                widening_factor=4,\n",
    "            )\n",
    "        )\n",
    "        return model_fn(x, is_training=is_training)\n",
    "\n",
    "params = model.init(rng, x, is_training=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_key(key: str):\n",
    "    to_remove = [\"bias\", \"LayerNorm\", \"layer_norm\", \"positional\", \"/b\", \"unembed\"]\n",
    "    return any(s in key for s in to_remove)\n",
    "\n",
    "\n",
    "params = flax.traverse_util.flatten_dict(params, sep=\"/\")\n",
    "params = {k: np.array(v) for k, v in params.items() if not filter_key(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json_tricks as json\n",
    "# \n",
    "# def save_dict_to_json(dictionary, filename):\n",
    "#     with open(filename, 'w') as file:\n",
    "#         json.dump(dictionary, file, indent=4)\n",
    "# \n",
    "# def load_dict_from_json(filename):\n",
    "#     with open(filename, 'r') as file:\n",
    "#         return json.load(file)\n",
    "# mup_params_name = \"mup_flax_params.json\"\n",
    "# sp_params_name = \"sp_haiku_params.json\"\n",
    "# \n",
    "# \n",
    "# \n",
    "# #save_dict_to_json(params, mup_params_name)\n",
    "# #save_dict_to_json(params, sp_params_name)\n",
    "# mup_params = load_dict_from_json(mup_params_name)\n",
    "# sp_params = load_dict_from_json(sp_params_name)\n",
    "# sp_vars = jax.tree_map(lambda w: jnp.var(w).item(), sp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*sp_vars.keys(), sep='\\n')\n",
    "# print(*mup_vars.keys(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_vars = {\n",
    " 'params/input/embedding/kernel': 0.07694920897483826,\n",
    " 'params/transformer/TransformerBlock_0/self_attention/query/kernel': 0.004992244299501181,\n",
    " 'params/transformer/TransformerBlock_0/self_attention/key/kernel': 0.004987955093383789,\n",
    " 'params/transformer/TransformerBlock_0/self_attention/value/kernel': 0.005008809268474579,\n",
    " 'params/transformer/TransformerBlock_0/self_attention/out/kernel': 0.005000918637961149,\n",
    " 'params/transformer/TransformerBlock_0/Dense_0/kernel': 0.005005593877285719,\n",
    " 'params/transformer/TransformerBlock_0/Dense_1/kernel': 0.0012498314026743174\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mup_vars = jax.tree_map(lambda w: jnp.var(w).item(), params)\n",
    "mup_vars = dict(mup_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params/input/embedding/kernel': 1.007582270365108,\n",
       " 'params/transformer/TransformerBlock_0/Dense_0/kernel': 1.0015089579236829,\n",
       " 'params/transformer/TransformerBlock_0/Dense_1/kernel': 0.9982550014937603,\n",
       " 'params/transformer/TransformerBlock_0/self_attention/key/kernel': 0.9978120249202611,\n",
       " 'params/transformer/TransformerBlock_0/self_attention/out/kernel': 1.0042500702033048,\n",
       " 'params/transformer/TransformerBlock_0/self_attention/query/kernel': 1.0010213052033996,\n",
       " 'params/transformer/TransformerBlock_0/self_attention/value/kernel': 0.9983570689426814}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x, y: x / y, sp_vars, mup_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slkfdsk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m slkfdsk\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slkfdsk' is not defined"
     ]
    }
   ],
   "source": [
    "slkfdsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta_model/linear/w': 0.2774,\n",
       " 'transformer/multi_head_attention/query/w': 0.07066,\n",
       " 'transformer/multi_head_attention/key/w': 0.07063,\n",
       " 'transformer/multi_head_attention/value/w': 0.07077,\n",
       " 'transformer/multi_head_attention/linear/w': 0.07072,\n",
       " 'transformer/linear/w': 0.07075,\n",
       " 'transformer/linear_1/w': 0.03535,\n",
       " 'meta_model/linear_1/w': 0.04401}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds = jax.tree_map(lambda w: jnp.std(w), loaded_params)\n",
    "stds = {k: v.item() for k, v in stds.items()}\n",
    "stds = {k: round(v, 5) for k, v in stds.items()}\n",
    "stds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
