{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from meta_transformer import torch_utils, module_path, on_cluster\n",
    "import os\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from meta_transformer.data import split_data\n",
    "import numpy as np\n",
    "import chex\n",
    "from tqdm import tqdm\n",
    "from meta_transformer import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dir():\n",
    "    if not on_cluster:\n",
    "        dpath = os.path.join(module_path, \"data/david_backdoors\")  # local\n",
    "    else:\n",
    "        dpath = \"/rds/user/lsl38/rds-dsk-lab-eWkDxBhxBrQ/model-zoo/\"  \n",
    "    return dpath\n",
    "\n",
    "\n",
    "LAYERS_TO_PERMUTE = [f'Conv2d_{i}' for i in range(6)] + ['Dense_6']\n",
    "\n",
    "\n",
    "def test_checkpoint_data_multiproc(data_dir):\n",
    "    data_dir = os.path.join(data_dir, \"test\")\n",
    "    inputs_dir = os.path.join(data_dir, \"inputs\")\n",
    "    targets_dir = os.path.join(data_dir, \"targets\")\n",
    "    architecture = torch_utils.CNNMedium()\n",
    "    inputs, targets, get_pytorch_model = torch_utils.load_pairs_of_models(\n",
    "        model=architecture,\n",
    "        data_dir1=inputs_dir,\n",
    "        data_dir2=targets_dir,\n",
    "        num_models=10,\n",
    "        max_workers=10,\n",
    "        prefix1=\"clean\",\n",
    "        prefix2=\"clean\",\n",
    "    )\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pairs of models from:\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/test/inputs\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/test/targets\n"
     ]
    }
   ],
   "source": [
    "models, check_models = test_checkpoint_data_multiproc(data_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(models):\n",
    "    for j, c in enumerate(check_models):\n",
    "        if i == j:\n",
    "            chex.assert_trees_all_close(m, c)\n",
    "        else:\n",
    "            try:\n",
    "                chex.assert_trees_all_close(m, c)\n",
    "                raise AssertionError(f\"Models {i} and {j} are the same!\")\n",
    "            except AssertionError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "layers_to_permute = [f'Conv2d_{i}' for i in range(6)] + ['Dense_6']\n",
    "loader = preprocessing.DataLoader(models, check_models,\n",
    "                                batch_size=2,\n",
    "                                rng=rng,\n",
    "                                max_workers=1,\n",
    "                                augment=True,\n",
    "                                layers_to_permute=layers_to_permute,\n",
    "                                skip_last_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load real model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    ndata: int = 10\n",
    "    dataset: str = 'mnist'\n",
    "    chunk_size: int = 256\n",
    "    bs: int = 2\n",
    "    augment: bool = True\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args = Args(\n",
    "    ndata=20,\n",
    "    dataset='cifar10',\n",
    ")\n",
    "\n",
    "if args.dataset == 'mnist':\n",
    "    architecture = torch_utils.CNNSmall()  # for MNIST\n",
    "elif args.dataset == 'cifar10':\n",
    "    architecture = torch_utils.CNNMedium()  # for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory allocated: 0.0 MB\n",
      "Initial memory reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2)} MB')\n",
    "print(f'Initial memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading pairs of models from:\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/cifar10/poison_noL1\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/cifar10/clean\n",
      "Data loading and processing took 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "#%%prun -s cumtime -l 30 -T 01_loading_data.txt\n",
    "if not on_cluster:\n",
    "    dpath = os.path.join(module_path, \"data/david_backdoors\")  # local\n",
    "else:\n",
    "    dpath = \"/rds/user/lsl38/rds-dsk-lab-eWkDxBhxBrQ/model-zoo/\"  \n",
    "\n",
    "model_dataset_paths = {\n",
    "    \"mnist\": \"mnist-cnns\",\n",
    "    \"cifar10\": \"cifar10\",\n",
    "    \"svhn\": \"svhn\",\n",
    "}\n",
    "\n",
    "model_dataset_paths = {\n",
    "    k: os.path.join(dpath, v) for k, v in model_dataset_paths.items()\n",
    "}\n",
    "\n",
    "inputs_dirnames = {\n",
    "    \"mnist\": \"poison\",\n",
    "    \"cifar10\": \"poison_noL1\",\n",
    "    \"svhn\": \"poison_noL1\",\n",
    "}\n",
    "\n",
    "inputs_dir = os.path.join(model_dataset_paths[args.dataset], inputs_dirnames[args.dataset])\n",
    "targets_dir = os.path.join(model_dataset_paths[args.dataset], \"clean\")\n",
    "\n",
    "print(\"Loading data...\")\n",
    "s = time()\n",
    "inputs, targets, get_pytorch_model = torch_utils.load_pairs_of_models(\n",
    "    model=architecture,\n",
    "    data_dir1=inputs_dir,\n",
    "    data_dir2=targets_dir,\n",
    "    num_models=args.ndata,\n",
    "    prefix2=\"clean\",\n",
    ")\n",
    "print(\"Data loading and processing took\", round(time() - s), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 19 models\n"
     ]
    }
   ],
   "source": [
    "print(\"loaded\", len(inputs), \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory allocated: 0.0 MB\n",
      "Current memory reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f'Current memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2)} MB')\n",
    "print(f'Current memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_inputs, train_targets, \n",
    "    val_inputs, val_targets) = split_data(inputs, targets, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n",
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n",
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n",
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n"
     ]
    }
   ],
   "source": [
    "from meta_transformer import preprocessing\n",
    "weights_std = 0.05\n",
    "\n",
    "init_batch = {\n",
    "    \"input\": train_inputs[:2],\n",
    "    \"target\": train_targets[:2],\n",
    "}\n",
    "\n",
    "init_batch = preprocessing.process_batch(init_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n",
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n",
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n",
      "Preprocessing parameters...\n",
      "starting to flatten\n",
      "yay flattened!\n"
     ]
    }
   ],
   "source": [
    "next_batch = {\n",
    "    \"input\": train_inputs[2:4],\n",
    "    \"target\": train_targets[2:4],\n",
    "}\n",
    "next_batch = preprocessing.process_batch(next_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay concurrency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/einops/parsing.py:136: RuntimeWarning: It is discouraged to use axes names that are keywords: in\n",
      "  warnings.warn(\"It is discouraged to use axes names that are keywords: {}\".format(name), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing parameters...\n",
      "starting to flatten\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok done great! yielding futures as they complete...\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mstop here\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: stop here"
     ]
    }
   ],
   "source": [
    "raise Exception(\"stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS_TO_PERMUTE == [f'Conv2d_{i}' for i in range(6)] + ['Dense_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/einops/parsing.py:136: RuntimeWarning: It is discouraged to use axes names that are keywords: in\n",
      "  warnings.warn(\"It is discouraged to use axes names that are keywords: {}\".format(name), RuntimeWarning)\n",
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/einops/parsing.py:136: RuntimeWarning: It is discouraged to use axes names that are keywords: in\n",
      "  warnings.warn(\"It is discouraged to use axes names that are keywords: {}\".format(name), RuntimeWarning)\n",
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/einops/parsing.py:136: RuntimeWarning: It is discouraged to use axes names that are keywords: in\n",
      "  warnings.warn(\"It is discouraged to use axes names that are keywords: {}\".format(name), RuntimeWarning)\n",
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/einops/parsing.py:136: RuntimeWarning: It is discouraged to use axes names that are keywords: in\n",
      "  warnings.warn(\"It is discouraged to use axes names that are keywords: {}\".format(name), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = preprocessing.DataLoader(train_inputs, train_targets,\n",
    "                            batch_size=args.bs,\n",
    "                            rng=np_rng,\n",
    "                            max_workers=None,\n",
    "                            augment=args.augment,\n",
    "                            #skip_last_batch=True,\n",
    "                            layers_to_permute=LAYERS_TO_PERMUTE,\n",
    "                            #chunk_size=args.chunk_size,\n",
    "                            )\n",
    "\n",
    "val_loader = preprocessing.DataLoader(val_inputs, val_targets,\n",
    "                        batch_size=args.bs,\n",
    "                        rng=np_rng,\n",
    "                        max_workers=None,\n",
    "                        augment=False,\n",
    "                        skip_last_batch=False,\n",
    "                        chunk_size=args.chunk_size,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sldkflsjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "valdata = []\n",
    "for batch in tqdm(val_loader):\n",
    "    state, val_metrics, aux = updater.compute_val_metrics(\n",
    "        state, batch)\n",
    "    if args.validate_output:  # validate depoisoning\n",
    "        rmetrics = get_reconstruction_metrics(aux[\"outputs\"])\n",
    "        val_metrics.update(rmetrics)\n",
    "    valdata.append(val_metrics)\n",
    "\n",
    "if len(valdata) == 0:\n",
    "    raise ValueError(\"Validation data is empty.\")\n",
    "val_metrics_means = jax.tree_map(lambda *x: np.mean(x), *valdata)\n",
    "val_metrics_means.update({\"epoch\": epoch, \"step\": state.step})\n",
    "logger.log(state, val_metrics_means, force_log=True)\n",
    "if stop_training:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
