{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauro/.virtualenvs/meta-models/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from meta_transformer import torch_utils, module_path, on_cluster\n",
    "import os\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from meta_transformer.data import split_data\n",
    "import numpy as np\n",
    "import chex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dir():\n",
    "    if not on_cluster:\n",
    "        dpath = os.path.join(module_path, \"data/david_backdoors\")  # local\n",
    "    else:\n",
    "        dpath = \"/rds/user/lsl38/rds-dsk-lab-eWkDxBhxBrQ/model-zoo/\"  \n",
    "    return dpath\n",
    "\n",
    "\n",
    "LAYERS_TO_PERMUTE = [f'Conv2d_{i}' for i in range(6)] + ['Dense_6']\n",
    "\n",
    "\n",
    "def test_checkpoint_data_multiproc(data_dir):\n",
    "    data_dir = os.path.join(data_dir, \"test\")\n",
    "    inputs_dir = os.path.join(data_dir, \"inputs\")\n",
    "    targets_dir = os.path.join(data_dir, \"targets\")\n",
    "    architecture = torch_utils.CNNMedium()\n",
    "    inputs, targets, get_pytorch_model = torch_utils.load_pairs_of_models(\n",
    "        model=architecture,\n",
    "        data_dir1=inputs_dir,\n",
    "        data_dir2=targets_dir,\n",
    "        num_models=100,\n",
    "        max_workers=100,\n",
    "        prefix1=\"clean\",\n",
    "        prefix2=\"clean\",\n",
    "    )\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pairs of models from:\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/test/inputs\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/test/targets\n"
     ]
    }
   ],
   "source": [
    "models, check_models = test_checkpoint_data_multiproc(data_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(models):\n",
    "    for j, c in enumerate(check_models):\n",
    "        if i == j:\n",
    "            chex.assert_trees_all_close(m, c)\n",
    "        else:\n",
    "            try:\n",
    "                chex.assert_trees_all_close(m, c)\n",
    "                raise AssertionError(f\"Models {i} and {j} are the same!\")\n",
    "            except AssertionError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load real model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    ndata: int = 10\n",
    "    dataset: str = 'mnist'\n",
    "    chunk_size: int = 256\n",
    "\n",
    "args = Args()\n",
    "\n",
    "#args = Args(\n",
    "#    ndata=1000,\n",
    "#    dataset='cifar10',\n",
    "#)\n",
    "\n",
    "if args.dataset == 'mnist':\n",
    "    architecture = torch_utils.CNNSmall()  # for MNIST\n",
    "elif args.dataset == 'cifar10':\n",
    "    architecture = torch_utils.CNNMedium()  # for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory allocated: 0.0 MB\n",
      "Initial memory reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2)} MB')\n",
    "print(f'Initial memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading pairs of models from:\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/mnist-cnns/poison_noL1reg\n",
      "/home/lauro/projects/meta-models/meta-transformer/data/david_backdoors/mnist-cnns/poison\n",
      "Data loading and processing took 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "#%%prun -s cumtime -l 30 -T 01_loading_data.txt\n",
    "if not on_cluster:\n",
    "    dpath = os.path.join(module_path, \"data/david_backdoors\")  # local\n",
    "    # use for testing with small dataset sizes (only works if rds storage is mounted):\n",
    "    # dpath = os.path.join(module_path, \"/home/lauro/rds/model-zoo/\")\n",
    "else:\n",
    "    dpath = \"/rds/user/lsl38/rds-dsk-lab-eWkDxBhxBrQ/model-zoo/\"  \n",
    "\n",
    "model_dataset_paths = {\n",
    "    \"mnist\": \"mnist-cnns\",\n",
    "    \"cifar10\": \"cifar10\",\n",
    "    \"svhn\": \"svhn\",\n",
    "}\n",
    "\n",
    "model_dataset_paths = {\n",
    "    k: os.path.join(dpath, v) for k, v in model_dataset_paths.items()\n",
    "}\n",
    "\n",
    "inputs_dirnames = {\n",
    "    \"mnist\": \"poison_noL1reg\",\n",
    "    #\"mnist\": \"poison\",\n",
    "    #\"cifar10\": \"poison_noL1\",\n",
    "    \"cifar10\": \"poison_simple\",\n",
    "    #\"cifar10\": \"poison_easy6_alpha_50\",\n",
    "    \"svhn\": \"poison_noL1\",\n",
    "}\n",
    "\n",
    "\n",
    "#print(\"Loading data...\")\n",
    "#s = time()\n",
    "#inputs, targets, get_pytorch_model = torch_utils.load_input_and_target_weights(\n",
    "#    model=architecture,\n",
    "#    num_models=args.ndata,\n",
    "#    data_dir=model_dataset_paths[args.dataset],\n",
    "#    inputs_dirname=inputs_dirnames[args.dataset],\n",
    "#    targets_dirname=\"clean\",\n",
    "#)\n",
    "#print(\"Data loading and processing took\", round(time() - s), \"seconds.\")\n",
    "\n",
    "\n",
    "inputs_dir = os.path.join(model_dataset_paths[args.dataset], inputs_dirnames[args.dataset])\n",
    "targets_dir = os.path.join(model_dataset_paths[args.dataset], \"poison\")\n",
    "\n",
    "print(\"Loading data...\")\n",
    "s = time()\n",
    "inputs, targets, get_pytorch_model = torch_utils.load_pairs_of_models(\n",
    "    model=architecture,\n",
    "    data_dir1=inputs_dir,\n",
    "    data_dir2=targets_dir,\n",
    "    num_models=args.ndata,\n",
    "    prefix2=\"poison\",\n",
    ")\n",
    "print(\"Data loading and processing took\", round(time() - s), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_pytorch_model(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 9 models\n"
     ]
    }
   ],
   "source": [
    "print(\"loaded\", len(inputs), \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory allocated: 0.0 MB\n",
      "Current memory reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f'Current memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2)} MB')\n",
    "print(f'Current memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(train_inputs, train_targets, \n",
    "    val_inputs, val_targets) = split_data(inputs, targets, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_transformer import preprocessing\n",
    "weights_std = 0.05\n",
    "\n",
    "init_batch = {\n",
    "    \"input\": train_inputs[:2],\n",
    "    \"target\": train_targets[:2],\n",
    "}\n",
    "\n",
    "init_batch = preprocessing.process_batch(\n",
    "    init_batch, augment=False, data_std=weights_std, chunk_size=args.chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory allocated: 0.0 MB\n",
      "Current memory reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f'Current memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2)} MB')\n",
    "print(f'Current memory reserved: {torch.cuda.memory_reserved() / (1024 ** 2)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Conv2d_0', 'Conv2d_1', 'Linear_2', 'Linear_3'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs[0]['Conv2d_0']['w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
